<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Quants R Us</title>
    <link>https://quantsrus.github.io/post/</link>
    <description>Recent content in Posts on Quants R Us</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Aug 2022 21:50:22 +0200</lastBuildDate><atom:link href="https://quantsrus.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Efficient Finite Difference Grid Stretching for Finance</title>
      <link>https://quantsrus.github.io/post/efficient_finite_difference_grid_stretching/</link>
      <pubDate>Sun, 28 Aug 2022 21:50:22 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/efficient_finite_difference_grid_stretching/</guid>
      <description>Partial differential equations sometimes have critical points where the solution or some of its derivatives are discontinuous. The simplest example is a discontinuity in the initial condition. It is well known that those decrease the accuracy of finite difference methods. A common remedy is to stretch the grid, such that many more grid points are present near the critical points, and fewer where the solution is deemed smooth. An alternative solution is to insert points such that the discontinuities fall in the middle of two grid points.</description>
    </item>
    
    <item>
      <title>Klein summation and Monte-Carlo simulations</title>
      <link>https://quantsrus.github.io/post/klein_sum_and_monte_carlo_statistics/</link>
      <pubDate>Sun, 28 Aug 2022 21:50:22 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/klein_sum_and_monte_carlo_statistics/</guid>
      <description>The straightforward and naive ways of calculating the mean and variance of the samples of a Monte-Carlo simulation may introduce artificial errors in the results. Monte-Carlo simulations are inherently parallelisable. It is possible and often desirable to run such a parallel simulation using a consistent set of random numbers, such that the results of the simulation are reproducible. However, the operating system scheduling of the simulation on the multiple cores or CPUs implies some randomness in the order of the completed batches of results, due to the varying load of the CPUs.</description>
    </item>
    
    <item>
      <title>Exponential B-Spline Collocation and Julia Automatic Differentiation</title>
      <link>https://quantsrus.github.io/post/exp_b_spline_collocation_autodiff/</link>
      <pubDate>Tue, 12 Oct 2021 21:50:22 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/exp_b_spline_collocation_autodiff/</guid>
      <description>I just added the exponential B-spline collocation to produce a flexible arbitrage-free interpolation of implied-volatilities to the book related github repo. I had written the polynomial collocation before, at the request of a reader of my book. The polynomial collocation is often what you will want to use but it has a few drawbacks sometimes:
 It may not always fit well enough given the limited number of free-parameters. It sometimes produces artificial spikes in the extrapolation, even if the use of a specific regularization helps to address this.</description>
    </item>
    
    <item>
      <title>The Cos Method, Go and Julia</title>
      <link>https://quantsrus.github.io/post/cos_method_go_julia/</link>
      <pubDate>Mon, 18 Jan 2021 21:50:22 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/cos_method_go_julia/</guid>
      <description>As I am preparing the third edition of my book (which I plan to complete this month), I am also moving some of the algorithms I coded in the Go language to Julia and at the same time make those public.
For the Cos method of Fang and Oosterlee, I actually had already written some code in Julia, to provide an implementation that supports arbitrary precision. Recently, I was reviewing some of the book content on vanilla option pricing under the Heston stochastic volatility model, to make it more up-to-date.</description>
    </item>
    
    <item>
      <title>The Fastest Implied Volatility Algorithms, Go vs. Julia</title>
      <link>https://quantsrus.github.io/post/implied_volatility_algorithms_go_julia/</link>
      <pubDate>Wed, 02 Dec 2020 21:50:22 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/implied_volatility_algorithms_go_julia/</guid>
      <description>In my book, I took a close look at the idea presented in the Chase the Devil blog to combine the very good initial Black-Scholes implied volatility guess of Dan Stefanica and Rados Radoicic with a relatively simple solver. The blogger recently criticized some new book, which clumsily tries to sell that idea as a novel algorithm, and presents fully inconsistent timing to assert that his new algorithm is much better.</description>
    </item>
    
    <item>
      <title>Why did I choose the Go language?</title>
      <link>https://quantsrus.github.io/post/why_did_i_choose_the_go_language/</link>
      <pubDate>Wed, 17 Jul 2019 15:50:22 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/why_did_i_choose_the_go_language/</guid>
      <description>Recently, a reader of my book asked me &amp;ldquo;why did you choose the Go language?&amp;rdquo;. I quickly mention it in the tools of the trade appendix. I will give a more elaborate answer to this question, and to the related question &amp;ldquo;would you still choose Go today?&amp;rdquo;.
Some of my initial motivations to choose the Go language were:
 Fast compilation time (almost instant). Fast run-time: at least on par with Java (it actually improved over the years).</description>
    </item>
    
    <item>
      <title>Julia and Python for the RBF collocation of a 2D PDE with multiple precision arithmetic</title>
      <link>https://quantsrus.github.io/post/rbf_collocation_of_2d_pde_and_precision/</link>
      <pubDate>Fri, 24 May 2019 15:50:22 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/rbf_collocation_of_2d_pde_and_precision/</guid>
      <description>I was curious about using Julia, mainly to access the Arb library for arbitrary precision linear algebra. It is written by the author of Python mpmath library, but its principle is quite different, and from the author&amp;rsquo;s blog, it is supposed to be much faster. Then, I also found python + numpy + scipy not so great at building large sparse matrices. It seemed relatively slow to me, numba does not work with scipy sparse matrices.</description>
    </item>
    
    <item>
      <title>Constraints in the Levenberg-Marquardt least-squares optimization</title>
      <link>https://quantsrus.github.io/post/constraints-in-levenberg-marquardt-least-squares-optimization/</link>
      <pubDate>Mon, 08 Oct 2018 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/constraints-in-levenberg-marquardt-least-squares-optimization/</guid>
      <description>The standard Levenberg-Marquardt optimizer does not support box constraints. This is, for example the case for good MINPACK lmdif/lmder implementations that many optimization libraries use underneath. But in practice, it is often useful to limit the range of the variables, often because the objective might not be defined everywhere.
The R minpack.lm CRAN package allows the user to specify box constraints. How do they do it?
They choose a very straightforward projection approach to enforce the constraints: if the guess is outside the box, they place it at the closest boundary (see the source code).</description>
    </item>
    
    <item>
      <title>The state of open-source quadratic programming convex optimizers</title>
      <link>https://quantsrus.github.io/post/state_of_convex_quadratic_programming_solvers/</link>
      <pubDate>Tue, 24 Jul 2018 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/state_of_convex_quadratic_programming_solvers/</guid>
      <description>I explore here a few open-source optimizers on a relatively simple problem of finding a good convex subset, but with many constraints: 30104 constraints for essentially 174 variables. My particular problem can be easily expressed in the form of a quadratic programming problem.
Java  Ojalgo: very difficult to setup properly in the absence of documentation for convex minimization. I suspect it does not work well, as it has a tendency to never finish.</description>
    </item>
    
    <item>
      <title>Staying arbitrage-free with Andreasen-Huge one-step interpolation</title>
      <link>https://quantsrus.github.io/post/staying-arbitrage-free-with-andreasen-huge-volatility-interpolation/</link>
      <pubDate>Thu, 08 Mar 2018 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/staying-arbitrage-free-with-andreasen-huge-volatility-interpolation/</guid>
      <description>Not long ago, I wrote a post about Andreasen-Huge arbitrage-free volatility interpolation method, showing that using a spline for the one-step local volatility instead of a piecewise-constant (or better, a piecewise-linear) function was not necessarily a great idea.
What we get out of Andreasen-Huge method, is a list of discrete option prices. What about option prices for strikes not on the grid?
In this case we still need some kind of interpolation.</description>
    </item>
    
    <item>
      <title>On the Quality of Research Publications</title>
      <link>https://quantsrus.github.io/post/on-the-quality-of-research-publications/</link>
      <pubDate>Wed, 28 Feb 2018 20:56:42 +0100</pubDate>
      
      <guid>https://quantsrus.github.io/post/on-the-quality-of-research-publications/</guid>
      <description>I spent the last week-end to review a paper for the journal Expert Systems with Applications. It was a paper on a variant of Spider Monkey Optimization, which is in the same spirit as differential evolution or particle swarm optimization. Yes, it could be added to the list of esoteric optimizers at the end of this Quants R Us post.
While the manuscript was relatively interesting in itself, and there was definitely some non-trivial amount of work behind it, it was riddled with errors: in the equations, in the algorithms, in the text, in the examples.</description>
    </item>
    
    <item>
      <title>Webassembly still fragile</title>
      <link>https://quantsrus.github.io/post/webassembly_still_fragile/</link>
      <pubDate>Tue, 05 Dec 2017 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/webassembly_still_fragile/</guid>
      <description>As I am preparing the website for my upcoming book on equity derivatives models, I played around with webassembly to run some C++ code from your web browser. In order to do that, I rely on emscripten, which seems to be the most advanced toolkit to generate webassembly code.
I did not expect the webassembly toolchain to be so fragile. At first I had trouble using some specific C code (not so complicated) from javascript through emscripten: there are multiple ways to do it.</description>
    </item>
    
    <item>
      <title>Particle Swarm Optimization on Heston Small-Time Expansion</title>
      <link>https://quantsrus.github.io/post/particle_swarm_optimization_heston_calibration/</link>
      <pubDate>Thu, 06 Jul 2017 07:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/particle_swarm_optimization_heston_calibration/</guid>
      <description>This is a sequel to my previous post on Particle Swarm Optimization. Here, I look at the problem of &amp;ldquo;calibrating&amp;rdquo; a Heston small-time expansion, the one from Forde &amp;amp; Jacquier). This can be useful to find a good initial guess for the exact Heston calibration, computed with much costlier characteristic function Fourier numerical integration.
Unfortunately, as we see below, with the contour plots of the objective function (the RMSE in volatilities against market quotes), the problem is much less well behaved with the small-time expansion than with the numerical integration.</description>
    </item>
    
    <item>
      <title>Particle Swarm Optimization</title>
      <link>https://quantsrus.github.io/post/particle_swarm_optimization/</link>
      <pubDate>Fri, 30 Jun 2017 07:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/particle_swarm_optimization/</guid>
      <description>In my previous post, I looked at simulated annealing to calibrate Heston. A less well-known and more fancy global minimizer is the particle swarm optimization (PSO). I stumbled upon it by accident through a youtube presentation from James McCaffrey. He shows a small python algorithm that solves the travelling salesman problem.
Intrigued, I started to read papers on it. It turns out there are a lot of publications on PSO, not always of great quality.</description>
    </item>
    
    <item>
      <title>Differential evolution vs. Simulated annealing</title>
      <link>https://quantsrus.github.io/post/differential_evolution_vs_simulated_annealing/</link>
      <pubDate>Wed, 21 Jun 2017 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/differential_evolution_vs_simulated_annealing/</guid>
      <description>The differential evolution (DE) algorithm is somewhat popular in quantitative finance, for example to calibrate stochastic volatility models such as Heston. There are a few parameters to setup properly but in general, it is not too difficult to find those, and the algorithm works well on many different problems.
An older technique, much more popular in physics is simulated annealing (SA). There are few papers on its use for stochastic volatility calibration, most don&amp;rsquo;t find the technique competitive (the ASA algorithm appear very slow in this paper from Ricardo Crisostomo, more standard SA is worse than a random search according to ManWo Ng) or even usable (see Jorg Kienitz book).</description>
    </item>
    
    <item>
      <title>A spline to fill the gaps with Andreasen-Huge one-step method</title>
      <link>https://quantsrus.github.io/post/andreasen_huge_spline/</link>
      <pubDate>Thu, 11 May 2017 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/andreasen_huge_spline/</guid>
      <description>I recently stumbled upon a blog which suggested to not stay flat with Andreasen-Huge arbitrage free volatility interpolation method. The paper from Andreasen and Huge specifies a piecewise constant (single-step) local volatility where the number of constants matches the number of market option prices.
The blog post shows eventual unstability with the piecewise constant approach, not visible with a linear interpolation. I wondered then if we should not go to the next level: use a spline on N values where N is the number of market options prices.</description>
    </item>
    
  </channel>
</rss>
